{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b19bfaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "P = [\n",
    "    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],\n",
    "    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "]\n",
    "\n",
    "P = np.array(P)\n",
    "\n",
    "rewards = [-1, -2, -2, 10, 1, 0] # rewards for states 0 to 5\n",
    "gamma = 0.5 # discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3a5b9d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_return(start_index, chain, gamma):\n",
    "    G = 0\n",
    "    for i in reversed(range(start_index, len(chain))): # from the end to the stratt_index of chain\n",
    "        G = gamma * G + rewards[chain[i] - 1] # -1 for zero-based index\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2835b817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return G starting from index 0 of chain: -2.500000\n"
     ]
    }
   ],
   "source": [
    "chain = [1, 2, 3, 6]\n",
    "start_index = 0\n",
    "G = compute_return(start_index, chain, gamma)\n",
    "print('Return G starting from index %d of chain: %f' % (start_index, G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1927a6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(P, rewards, gamma, states_num):\n",
    "    rewards = np.array(rewards).reshape((-1, 1))\n",
    "    # np.linalg.inv: matrix inverse\n",
    "    # np.eye: identity matrix\n",
    "    value = np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P), rewards)\n",
    "\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1820ca66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function V: \n",
      " [[-2.01950168]\n",
      " [-2.21451846]\n",
      " [ 1.16142785]\n",
      " [10.53809283]\n",
      " [ 3.58728554]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "V = compute(P, rewards, gamma, states_num=6)\n",
    "print('Value function V: \\n', V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f9071819",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = ['s1', 's2', 's3', 's4', 's5'] # set of states\n",
    "A = ['Keep s1', 'Go s1', 'Go s2', 'Go s3', 'Go s4', 'Go s5', 'Go in Prob'] # set of actions\n",
    "P = { # state transition func\n",
    "    \"s1-Keep s1-s1\": 1.0,\n",
    "    \"s1-Go s2-s2\": 1.0,\n",
    "    \"s2-Go s1-s1\": 1.0,\n",
    "    \"s2-Go s3-s3\": 1.0,\n",
    "    \"s3-Go s4-s4\": 1.0,\n",
    "    \"s3-Go s5-s5\": 1.0,\n",
    "    \"s4-Go s5-s5\": 1.0,\n",
    "    \"s4-Go in Prob-s2\": 0.2,\n",
    "    \"s4-Go in Prob-s3\": 0.4,\n",
    "    \"s4-Go in Prob-s4\": 0.4,\n",
    "}\n",
    "R = { # reward func\n",
    "    \"s1-Keep s1\": -1,\n",
    "    \"s1-Go s2\": 0,\n",
    "    \"s2-Go s1\": -1,\n",
    "    \"s2-Go s3\": -2,\n",
    "    \"s3-Go s4\": -2,\n",
    "    \"s3-Go s5\": 0,\n",
    "    \"s4-Go s5\": 10,\n",
    "    \"s4-Go in Prob\": 1,\n",
    "}\n",
    "gamma = 0.5 # discount factor\n",
    "MDP = (S, A, P, R, gamma) # markov decision process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2f81c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy 1, Random Policy\n",
    "Pi_1 = {\n",
    "    \"s1-Keep s1\": 0.5,\n",
    "    \"s1-Go s2\": 0.5,\n",
    "    \"s2-Go s1\": 0.5,\n",
    "    \"s2-Go s3\": 0.5,\n",
    "    \"s3-Go s4\": 0.5,\n",
    "    \"s3-Go s5\": 0.5,\n",
    "    \"s4-Go s5\": 0.5,\n",
    "    \"s4-Go in Prob\": 0.5,\n",
    "}\n",
    "# Policy 2\n",
    "Pi_2 = {\n",
    "    \"s1-Keep s1\": 0.6,\n",
    "    \"s1-Go s2\": 0.4,\n",
    "    \"s2-Go s1\": 0.3,\n",
    "    \"s2-Go s3\": 0.7,\n",
    "    \"s3-Go s4\": 0.5,\n",
    "    \"s3-Go s5\": 0.5,\n",
    "    \"s4-Go s5\": 0.1,\n",
    "    \"s4-Go in Prob\": 0.9,\n",
    "}\n",
    "\n",
    "def join(str1, str2):\n",
    "    return str1 + '-' + str2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "229404e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Value of MDP [[-1.22555411]\n",
      " [-1.67666232]\n",
      " [ 0.51890482]\n",
      " [ 6.0756193 ]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.5\n",
    "# MDP to MRP\n",
    "P_from_mdp_to_mrp = [\n",
    "    [0.5, 0.5, 0.0, 0.0, 0.0],\n",
    "    [0.5, 0.0, 0.5, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.5, 0.5],\n",
    "    [0.0, 0.1, 0.2, 0.2, 0.5],\n",
    "    [0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "]\n",
    "P_from_mdp_to_mrp = np.array(P_from_mdp_to_mrp)\n",
    "R_from_mdp_to_mrp = [-0.5, -1.5, -1.0, 5.5, 0]\n",
    "\n",
    "V = compute(P_from_mdp_to_mrp, R_from_mdp_to_mrp, gamma, 5)\n",
    "print(\"State Value of MDP\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "7b8f692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Monte Carlo Simulation for Policy Evaluation\n",
    "def sample(MDP, Pi, timestep_max, number):\n",
    "    S, A, P, R, gamma = MDP\n",
    "    episodes = []\n",
    "    for _ in range(number):\n",
    "        episode = []\n",
    "        timestep = 0\n",
    "        state = S[np.random.randint(4)] # s5 is terminal state\n",
    "        while timestep <= timestep_max and state != 's5':\n",
    "            timestep += 1\n",
    "            temp = 0\n",
    "            rand = np.random.rand()\n",
    "            for a in A: # choose action according to policy Pi\n",
    "                temp += Pi.get(join(state, a), 0) # get the prob of action a under state\n",
    "                if temp >= rand: # random choose action according to prob\n",
    "                    action = a\n",
    "                    reward = R.get(join(state, a), 0)\n",
    "                    break\n",
    "            temp = 0\n",
    "            rand = np.random.rand()\n",
    "            for s in S: # choose next state according to state transition func P\n",
    "                temp += P.get(join(join(state, action), s), 0)\n",
    "                if temp >= rand: # random choose next state according to prob\n",
    "                    next_state = s\n",
    "                    break\n",
    "            episode.append((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "        episodes.append(episode)\n",
    "\n",
    "    return episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "d1c24e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Firts Sequence of Episodes: \n",
      " [('s1', 'Keep s1', -1, 's1'), ('s1', 'Keep s1', -1, 's1'), ('s1', 'Go s2', 0, 's2'), ('s2', 'Go s1', -1, 's1'), ('s1', 'Keep s1', -1, 's1'), ('s1', 'Go s2', 0, 's2'), ('s2', 'Go s3', -2, 's3'), ('s3', 'Go s4', -2, 's4'), ('s4', 'Go s5', 10, 's5')]\n"
     ]
    }
   ],
   "source": [
    "episodes = sample(MDP, Pi_1, 20, 5)\n",
    "print('Firts Sequence of Episodes: \\n', episodes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "a872e2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'s1': -1.2253273289232065,\n",
       " 's2': -1.665252654265004,\n",
       " 's3': 0.5286840442642281,\n",
       " 's4': 6.099244267332785,\n",
       " 's5': 0}"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_policy(episodes, V, N, gamma):\n",
    "    for episode in episodes:\n",
    "        G = 0\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward, next_state = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            N[state] += 1\n",
    "            V[state] += (G - V[state]) / N[state] # incremental mean\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "timestep_max = 20\n",
    "episodes = sample(MDP, Pi_1, timestep_max, 1000)\n",
    "gamma = 0.5\n",
    "V = {s: 0 for s in S}\n",
    "N = {s: 0 for s in S}\n",
    "eval_policy(episodes, V, N, gamma)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "89b40e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occupancy measure under Pi_1: 0.116357\n",
      "Occupancy measure under Pi_2: 0.234478\n"
     ]
    }
   ],
   "source": [
    "def estimate_occupancy(episodes, state, action, timestep_max, gamma):\n",
    "    rho = 0\n",
    "    total_times = np.zeros(timestep_max) # number of all (state, action) occurred in episode at timestep t\n",
    "    occur_times = np.zeros(timestep_max) # times of (state, action) occurred in episode at timestep t\n",
    "    for episode in episodes:\n",
    "        for i in range(len(episode)):\n",
    "            (s, a, r, s_next) = episode[i]\n",
    "            total_times[i] += 1\n",
    "            if state == s and action == a:\n",
    "                occur_times[i] += 1\n",
    "    for t in reversed(range(timestep_max)):\n",
    "        if total_times[t] > 0:\n",
    "            rho += (gamma ** t) * (occur_times[t] / total_times[t])\n",
    "    \n",
    "    return (1 - gamma) * rho\n",
    "\n",
    "gamma = 0.5\n",
    "timestep_max = 1000\n",
    "\n",
    "episodes_pi1 = sample(MDP, Pi_1, timestep_max, 1000)\n",
    "episodes_pi2 = sample(MDP, Pi_2, timestep_max, 1000)\n",
    "\n",
    "rho_1 = estimate_occupancy(episodes_pi1, 's4', 'Go in Prob', timestep_max, gamma)\n",
    "rho_2 = estimate_occupancy(episodes_pi2, 's4', 'Go in Prob', timestep_max, gamma)\n",
    "print('Occupancy measure under Pi_1: %f' % rho_1)\n",
    "print('Occupancy measure under Pi_2: %f' % rho_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
