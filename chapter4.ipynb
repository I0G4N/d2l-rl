{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0942bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class CliffWalkingEnv:\n",
    "    \"\"\"Cliff Walking Environment\"\"\"\n",
    "    def __init__(self, ncol=12, nrow=4):\n",
    "        self.ncol = ncol\n",
    "        self.nrow = nrow\n",
    "        # state transition matrix P[state][action] = [(prob., next_state, reward, terminal)]\n",
    "        self.P = self.create_P()\n",
    "\n",
    "    def create_P(self):\n",
    "        # initialize state transition matrix\n",
    "        P = [[[] for j in range(4)] for i in range(self.ncol * self.nrow)]\n",
    "        # change[0] = up, change[1] = down, change[2] = left, change[3] = right\n",
    "        # [0, 0] is top-left corner\n",
    "        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]\n",
    "        for y in range(self.nrow):\n",
    "            for x in range(self.ncol):\n",
    "                for action in range(4):\n",
    "                    # if in cliff area or goal state, each action reward is 0\n",
    "                    if y == self.nrow - 1 and x > 0:\n",
    "                        P[y * self.ncol + x][action] = [(1, y * self.ncol + x, 0, True)]\n",
    "                        continue\n",
    "                    # other area\n",
    "                    next_x = min(self.ncol - 1, max(0, x + change[action][0]))\n",
    "                    next_y = min(self.nrow - 1, max(0, y + change[action][1]))\n",
    "                    next_state = next_y * self.ncol + next_x\n",
    "                    reward = -1\n",
    "                    terminal = False\n",
    "                    # if next state is cliff area or goal state\n",
    "                    if next_y == self.nrow - 1 and next_x > 0:\n",
    "                        terminal = True\n",
    "                        if next_x != self.ncol - 1: # cliff area\n",
    "                            reward = -100\n",
    "                    \n",
    "                    P[y * self.ncol + x][action] = [(1, next_state, reward, terminal)]\n",
    "\n",
    "        return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db3378f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    def __init__(self, env, theta, gamma):\n",
    "        self.env = env\n",
    "        self.v = [0] * self.env.ncol * self.env.nrow\n",
    "        self.policy = [[0.25, 0.25, 0.25, 0.25] for _ in range(self.env.ncol * self.env.nrow)]\n",
    "        self.theta = theta # threshold for stopping evaluation\n",
    "        self.gamma = gamma # discount factor\n",
    "\n",
    "    def policy_evaluation(self):\n",
    "        cnt = 1 # count of evaluation\n",
    "        while True:\n",
    "            max_diff = 0\n",
    "            new_v = [0] * self.env.ncol * self.env.nrow\n",
    "            for state in range(self.env.ncol * self.env.nrow):\n",
    "                q_sa_list = [] # list of q value for each state-action pair under state\n",
    "                for action in range(4):\n",
    "                    q_sa = 0\n",
    "                    for res in self.env.P[state][action]: # each possible outcome (there is only one in this env)\n",
    "                        prob, next_state, reward, terminal = res\n",
    "                        # $\\P(a|s) * (r(s, a) + \\gamma V(s') * (1 - termial))$ when terminal is True, V(s') cannot be added\n",
    "                        q_sa += prob * (reward + self.gamma * self.v[next_state] * (1 - terminal))\n",
    "                    q_sa_list.append(self.policy[state][action] * q_sa) # multiply by policy prob\n",
    "                new_v[state] = sum(q_sa_list)\n",
    "                max_diff = max(max_diff, abs(new_v[state] - self.v[state])) # record max diff of all states\n",
    "            self.v = new_v\n",
    "            if max_diff < self.theta: # reached threshold\n",
    "                break\n",
    "            cnt += 1\n",
    "        print(f'Policy Evaluation converged in {cnt} iterations.')\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        for state in range(self.env.ncol * self.env.nrow):\n",
    "            q_sa_list = [] # list of q value for each state-action pair under state\n",
    "            for action in range(4):\n",
    "                q_sa = 0\n",
    "                for res in self.env.P[state][action]:\n",
    "                    prob, next_state, reward, terminal = res\n",
    "                    q_sa += prob * (reward + self.gamma * self.v[next_state] * (1 - terminal))\n",
    "                q_sa_list.append(q_sa)\n",
    "            max_q = max(q_sa_list) # find max q value of all actions\n",
    "            cnt_max_q = q_sa_list.count(max_q) # count how many actions have the max q value\n",
    "            # average prob for all actions with max q value\n",
    "            self.policy[state] = [1 / cnt_max_q if q == max_q else 0 for q in q_sa_list]\n",
    "        print('Policy Improvement done.')\n",
    "\n",
    "        return self.policy\n",
    "    \n",
    "    def policy_iteration(self):\n",
    "        while True:\n",
    "            self.policy_evaluation()\n",
    "            old_policy = copy.deepcopy(self.policy)\n",
    "            new_policy = self.policy_improvement()\n",
    "            if old_policy == new_policy:\n",
    "                break\n",
    "        print('Policy Iteration done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "697cb07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Evaluation converged in 45 iterations.\n",
      "Policy Improvement done.\n",
      "Policy Evaluation converged in 50 iterations.\n",
      "Policy Improvement done.\n",
      "Policy Evaluation converged in 27 iterations.\n",
      "Policy Improvement done.\n",
      "Policy Evaluation converged in 12 iterations.\n",
      "Policy Improvement done.\n",
      "Policy Evaluation converged in 1 iterations.\n",
      "Policy Improvement done.\n",
      "Policy Iteration done.\n",
      "Value of States:\n",
      "-7.712 -7.458 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 \n",
      "-7.458 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 \n",
      "-7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 -1.000 \n",
      "-7.458  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 \n",
      "\n",
      "Policy:\n",
      "ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovoo \n",
      "ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovoo \n",
      "ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ovoo \n",
      "^ooo **** **** **** **** **** **** **** **** **** **** EEEE \n"
     ]
    }
   ],
   "source": [
    "def print_agent(agent: CliffWalkingEnv, action_meaning, disaster=[], end=[]):\n",
    "    print('Value of States:')\n",
    "    for y in range(agent.env.nrow):\n",
    "        for x in range(agent.env.ncol):\n",
    "            print(f\"{agent.v[y * agent.env.ncol + x]:6.3f}\", end=' ')\n",
    "        print()\n",
    "\n",
    "    print('\\nPolicy:')\n",
    "    for y in range(agent.env.nrow):\n",
    "        for x in range(agent.env.ncol):\n",
    "            if (y * agent.env.ncol + x) in disaster: # cliff\n",
    "                print('****', end=' ')\n",
    "            elif (y * agent.env.ncol + x) in end: # goal\n",
    "                print('EEEE', end=' ')\n",
    "            else:\n",
    "                action = agent.policy[y * agent.env.ncol + x]\n",
    "                policy_str = ''\n",
    "                for k in range(len(action_meaning)):\n",
    "                    policy_str += action_meaning[k] if action[k] > 0 else 'o'\n",
    "                print(f'{policy_str}', end=' ')\n",
    "        print()\n",
    "\n",
    "\n",
    "env = CliffWalkingEnv()\n",
    "agent = PolicyIteration(env, theta=0.01, gamma=0.9)\n",
    "agent.policy_iteration()\n",
    "print_agent(agent, ['^', 'v', '<', '>'], disaster=list(range(37, 47)), end=[47])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
